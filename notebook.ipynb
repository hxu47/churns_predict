{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classification: Predict if customers will get churned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://deepnote.com/project/d899ea24-a16c-48e2-9d9b-7483d07922b5#%2Fnotebook.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from category_encoders import *\n",
    "from sklearn.compose import *\n",
    "from sklearn.impute import *\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import balanced_accuracy_score \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import f1_score \n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_churn(row):\n",
    "    if row['Attrition_Flag'] == 'Existing Customer' :\n",
    "        return 0\n",
    "    if row['Attrition_Flag'] == 'Attrited Customer':\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train-test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[df.columns[2:]]\n",
    "y = pd.DataFrame()\n",
    "y['Attrition_Flag'] = df.apply(lambda row: label_churn(row), axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6075, 19) (2026, 19) (2026, 19)\n"
     ]
    }
   ],
   "source": [
    "#  Ensure validation set has same properties as test set (e.g., size)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature transformations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define catigorical variables groups and continuous variables groups \n",
    "# to fit into different pipeline\n",
    "cat_vars = ['Gender', 'Education_Level', 'Income_Category', 'Marital_Status', 'Card_Category']\n",
    "con_vars_1 = ['Total_Relationship_Count', 'Total_Trans_Ct']\n",
    "con_vars_2 = ['Customer_Age', 'Credit_Limit']\n",
    "\n",
    "# pipeline for continuous variables\n",
    "# fill null values with median then standardize\n",
    "con_pipe = Pipeline([('imputer', SimpleImputer(strategy='median', add_indicator=True)),\n",
    "                     ('scaler', StandardScaler())])\n",
    "\n",
    "# pipeline for categorical variables\n",
    "# fill null values with mode then do OneHotEncoder\n",
    "cat_pipe = Pipeline([('imputer', SimpleImputer(strategy='most_frequent', add_indicator=True)),\n",
    "                     ('ohe', OneHotEncoder(handle_unknown='ignore'))\n",
    "                     ])\n",
    "\n",
    "# put all feature transformations in a signle Pipeline\n",
    "preprocessing = ColumnTransformer([('categorical', cat_pipe,  cat_vars),\n",
    "                                   ('continuous',  con_pipe,  con_vars_1),\n",
    "                                   ('qt', QuantileTransformer(output_distribution='normal'), con_vars_2)\n",
    "                                   ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Algorithms & Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyEstimator(BaseEstimator):\n",
    "    \"Pass through class, methods are present but do nothing.\"\n",
    "    def fit(self): pass\n",
    "    def score(self): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    }
   ],
   "source": [
    "# Create space of candidate learning algorithms and their hyperparameters\n",
    "\n",
    "# Setup pipeline with DummyEstimator to cv search across algorithms\n",
    "pipe = Pipeline([(\"preprocessing\", preprocessing),\n",
    "                 ('pca', PCA()),\n",
    "                 ('clf', DummyEstimator())])\n",
    "\n",
    "# Go through each algorithm and a variety of hyperparameters\n",
    "search_space = [{'clf': [LogisticRegression()], # Actual Estimator\n",
    "                 'clf__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "                 'clf__C': np.logspace(0, 4, 10),\n",
    "                 'clf__class_weight': [None, 'balanced'],\n",
    "                 'clf__solver': ['newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "                 'pca__n_components': range(1, 15)},\n",
    "                \n",
    "                {'clf': [RandomForestClassifier()],  # Actual Estimator\n",
    "                 'clf__criterion': ['gini', 'entropy'],\n",
    "                 'clf__max_depth': [None, 5, 10, 15, 20, 25],\n",
    "                 'clf__min_samples_split': range(2,40),\n",
    "                 'clf__min_samples_leaf': range(1,30),\n",
    "                 'clf__max_features': ['auto', 'sqrt', 'log2'],\n",
    "                 'clf__max_leaf_nodes': (range(2,50)),\n",
    "                 'clf__max_samples': [None, 0.25, 0.33, 0.5, 0.67, 0.8],\n",
    "                 'clf__class_weight': ['balanced', 'balanced_subsample'],\n",
    "                 'pca__n_components': range(1, 15)},\n",
    "                \n",
    "                {'clf': [ExtraTreesClassifier()],\n",
    "                 'clf__criterion': ['gini', 'entropy'],\n",
    "                 'clf__max_depth': [None, 5, 10, 15, 20, 25],\n",
    "                 'clf__min_samples_split': range(2,40),\n",
    "                 'clf__min_samples_leaf': range(1,30),\n",
    "                 'clf__max_features': ['auto', 'sqrt', 'log2'],\n",
    "                 'clf__max_leaf_nodes': (range(2,50)),\n",
    "                 'clf__max_samples': [None, 0.25, 0.33, 0.5, 0.67, 0.8],\n",
    "                 'clf__class_weight': ['balanced', 'balanced_subsample'],\n",
    "                 'pca__n_components': range(1, 15)\n",
    "                },\n",
    "                \n",
    "                {'clf': [RidgeClassifier()],\n",
    "                 'clf__alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "                 'clf__class_weight': [None, 'balanced'],\n",
    "                 'clf__solver': ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'saga'],\n",
    "                 'pca__n_components': range(1, 15)\n",
    "                },\n",
    "                \n",
    "                {'clf': [KNeighborsClassifier()],\n",
    "                 'clf__weights': ['uniform', 'distance'],\n",
    "                 'clf__algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "                 'clf__leaf_size': range(1, 60),\n",
    "                 'clf__p' : [1, 2],\n",
    "                 'pca__n_components': range(1, 15)\n",
    "                },\n",
    "                \n",
    "                {'clf': [SVC()],\n",
    "                 'clf__kernel': ['liner', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "                 'clf__degree': range(1,10),\n",
    "                 'clf__gamma': ['scale', 'auto'],\n",
    "                 'pca__n_components': range(1, 15)\n",
    "                }\n",
    "                \n",
    "               ]\n",
    "\n",
    "\n",
    "clf_algos_rand = RandomizedSearchCV(estimator=pipe, \n",
    "                                    param_distributions=search_space, \n",
    "                                    n_iter=100,\n",
    "                                    cv=5, \n",
    "                                    n_jobs=-1,\n",
    "                                    verbose=1)\n",
    "\n",
    "# Grid search\n",
    "best_model = clf_algos_rand.fit(X_train, y_train.values.ravel());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(n_components=13)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.best_estimator_.get_params()['pca']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(class_weight='balanced_subsample', max_depth=25,\n",
       "                       max_features='sqrt', max_leaf_nodes=45, max_samples=0.25,\n",
       "                       min_samples_leaf=7)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View best model\n",
    "best_model.best_estimator_.get_params()['clf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fit the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {'class_weight': 'balanced_subsample', \n",
    "                   'max_depth': 25,\n",
    "                   'max_features': 'sqrt',\n",
    "                   'max_leaf_nodes': 45, \n",
    "                   'max_samples': 0.25,\n",
    "                   'min_samples_leaf': 7}\n",
    "\n",
    "pipe = Pipeline([(\"preprocessing\", preprocessing),\n",
    "                 ('pca', PCA(n_components=13)),\n",
    "                 ('etc', RandomForestClassifier(**hyperparameters))])\n",
    "\n",
    "pipe.fit(X_train, y_train.values.ravel());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8460019743336624"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy: correct classfication rate\n",
    "acc_test = accuracy_score(y_test.values.ravel(), y_pred)\n",
    "acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.855577003754181"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f1: that calculates the harmonic mean of the precision and recall\n",
    "f1_test  = f1_score(y_test.values.ravel(), y_pred, average='weighted')\n",
    "f1_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8909289724507117"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_recall_curve\n",
    "roc_auc_score(y_test.values.ravel(), pipe.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1483,  219],\n",
       "       [  93,  231]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute confusion matrix to evaluate the accuracy of a classification.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test.values.ravel(), y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the confusion matrix, the true negative and true positive is high, but the false positive is false negative is also not low. The model doesn't do a good job on predicting the right class, but its ability to correctly predict class 0 is better than its ability to correctly predict class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model I choose is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(class_weight='balanced_subsample', max_depth=25,\n",
       "                       max_features='sqrt', max_leaf_nodes=45, max_samples=0.25,\n",
       "                       min_samples_leaf=7)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe['etc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipe with steps and non-default hyperparameters:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessing',\n",
       "                 ColumnTransformer(transformers=[('categorical',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(add_indicator=True,\n",
       "                                                                                 strategy='most_frequent')),\n",
       "                                                                  ('ohe',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                  ['Gender', 'Education_Level',\n",
       "                                                   'Income_Category',\n",
       "                                                   'Marital_Status',\n",
       "                                                   'Card_Category']),\n",
       "                                                 ('continuous',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImpute...\n",
       "                                                                  ('scaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['Total_Relationship_Count',\n",
       "                                                   'Total_Trans_Ct']),\n",
       "                                                 ('qt',\n",
       "                                                  QuantileTransformer(output_distribution='normal'),\n",
       "                                                  ['Customer_Age',\n",
       "                                                   'Credit_Limit'])])),\n",
       "                ('pca', PCA(n_components=13)),\n",
       "                ('etc',\n",
       "                 RandomForestClassifier(class_weight='balanced_subsample',\n",
       "                                        max_depth=25, max_features='sqrt',\n",
       "                                        max_leaf_nodes=45, max_samples=0.25,\n",
       "                                        min_samples_leaf=7))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I built a pipeline to preprocess the data, and then use RandomizedSearch cross validation, I searched across six algorithms with specific lists of hyperparameters. I also performed automated hyperparameter search across to find the best n_componentsthe PCA . The result is that RandomForestClassifier works best for this dataset. Wrapping up the steps of preprocessing the data, principal component analysis and the final model into a pipeline, I train the model on the train set and evaluate it on the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics accuracy, f1, and auc roc are not very bad, however, from the confusion matrix, I can see the model didn't do a very good job. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps / future directions: \n",
    "+ Gather more data. \n",
    "+ The dataset is a little bit imbalanced, because it has only 19.07% of customers who have churned. Next step, we can use SMOTE or bootstrapping techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
